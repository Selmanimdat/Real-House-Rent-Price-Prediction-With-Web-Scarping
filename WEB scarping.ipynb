{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b7047b9",
   "metadata": {},
   "source": [
    "# WEB scarping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d81e1-99dc-4ca4-b74d-0615cc910552",
   "metadata": {},
   "source": [
    "İn this notebook we going to write a web scarping code for house rents scaring from<br>\n",
    "<a href=\"https://www.zingat.com/\" target=_blank>zingat.com</a>.  İn this part scarp only İstanbul data.<br>\n",
    "After that we develop a regression model for prediction istanbul house rent prices and make a webisite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0752f-cd6e-435f-83b2-30041c6bb532",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/6zM7JBq.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164fe6fb-97e5-4e48-b5a9-1659b4421a5c",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87070dc1-7a94-4552-95d1-1dbe18a00711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from undetected_chromedriver import Chrome\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab267b2-f1f5-4d0c-9f76-a02f7019e7a7",
   "metadata": {},
   "source": [
    "Firstly we will take all rent house adverts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6698d27a-9be8-4d16-a730-c0214a48ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for scarp adverts\n",
    "def take_adverts(url):\n",
    "    # Use undetected_chromedriver as uc\n",
    "    driver = Chrome()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait for JavaScript to execute and load the content\n",
    "        driver.implicitly_wait(10)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        links = soup.select('li.zl-card a.zl-card-inner')\n",
    "        return list(map(lambda link: link.get('href'), links))\n",
    "    except Exception as e:\n",
    "        print(f\"Hata oluştu: {e}\")\n",
    "    finally:\n",
    "        # Quit the driver to release system resources\n",
    "        driver.quit()\n",
    "\n",
    "def get_all_adverts(base_url, total_pages):\n",
    "    # Create all page URLs\n",
    "    urls = map(lambda page: base_url.format(page), range(1, total_pages + 1))\n",
    "    # Pull ads from all pages\n",
    "    all_links_lists = map(take_adverts, urls)\n",
    "    # Combine all ad links\n",
    "    all_links = list(itertools.chain.from_iterable(all_links_lists))\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04261687-8d8b-41ac-a95c-7e03fb5baa70",
   "metadata": {},
   "source": [
    "Aplication to function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc65a79-c67a-4764-8456-c6bbd0952a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL and Total Pages Number\n",
    "base_url = \"https://www.zingat.com/istanbul-kiralik?page={}\"\n",
    "total_pages = 48\n",
    "\n",
    "# Take all the ads\n",
    "all_adverts = get_all_adverts(base_url, total_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767c503-c017-47b3-87d8-1cc4db924d32",
   "metadata": {},
   "source": [
    "Secondly we will tek all data we want from all adverds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb6b29d-e0bd-4546-9698-ff182223227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(url):\n",
    "    try:\n",
    "        # Start the Chrome driver\n",
    "        driver = Chrome()\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for JavaScript to load\n",
    "        driver.implicitly_wait(10)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        # Extract li elements with class 'col-md-6'\n",
    "        li_elements = soup.find_all('li', class_='col-md-6')\n",
    "        \n",
    "        # Find the element with class \"listing-price\" and extract the text\n",
    "        price_element = soup.find('strong', itemprop='price')\n",
    "        price = price_element.text.strip() if price_element else \"N/A\"\n",
    "        \n",
    "        # Extract location data\n",
    "        location_element = soup.find('h2')\n",
    "        location = location_element.text.strip() if location_element else \"N/A\"\n",
    "        \n",
    "        # Extract information from each li element and add the price and location\n",
    "        data = {li.find('strong').text.strip(): li.find('span').text.strip() for li in li_elements}\n",
    "        data[\"Location\"] = location\n",
    "        data['Price'] = price\n",
    "        \n",
    "        # Create a DataFrame from the extracted data\n",
    "        df = pd.DataFrame([data])\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping {url}: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "    \n",
    "    finally:\n",
    "        # Close the driver\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ba23c-cdf7-4c4a-ae74-6c01aeef0a71",
   "metadata": {},
   "source": [
    "Usage of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787f180-2f64-4280-be3d-3699251bd867",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.DataFrame()\n",
    "for url in all_adverts:\n",
    "    try:\n",
    "        scarp_url=\"https://www.zingat.com\"+url\n",
    "        df=extract_data(scarp_url)\n",
    "        dataframe = pd.concat([dataframe, df], ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "try:\n",
    "    dataframe=pd.DataFrame(dataframe)\n",
    "    dataframe.shape\n",
    "    dataframe.head()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16a370-3c16-4246-badf-370e59faee21",
   "metadata": {},
   "source": [
    "Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddb18395",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"rent_houses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e9819-3d5d-4e5b-8ed3-fccb0c06150e",
   "metadata": {},
   "source": [
    "Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9010cb",
   "metadata": {},
   "source": [
    "-Zafer Acar Notes <br>-[https://chatgpt.com/]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
